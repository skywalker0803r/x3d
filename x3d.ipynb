{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "1C8-JugiIBI761mI8O-LfpZHMaE8Kp9jj",
      "authorship_tag": "ABX9TyOLodUBf73v3a62t9qAU+rT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skywalker0803r/x3d/blob/main/x3d.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMNTIClCCBWm",
        "outputId": "855b073c-c5b9-4cfb-f463-1e2f03a10761"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/144 [00:00<?, ?it/s]<ipython-input-32-4133571272>:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "<ipython-input-32-4133571272>:157: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.6182 | Train Acc: 0.6949\n",
            "Val   Loss: 0.5595 | Val   Acc: 0.7549\n",
            "‚úÖ Saved best model to Google Drive: /content/drive/MyDrive/epoch_1_valacc_0.7549.pt\n",
            "\n",
            "Epoch 2/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  26%|‚ñà‚ñà‚ñå       | 37/144 [02:10<07:47,  4.37s/it]"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from pytorchvideo.models.hub import x3d_xs\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "from torchvision.io import read_video\n",
        "from torchvision import transforms as T\n",
        "import cv2\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "import random\n",
        "from PIL import Image\n",
        "from torch.utils.data import ConcatDataset\n",
        "from torch.utils.data import Subset\n",
        "import copy\n",
        "from PIL import Image, ImageFilter\n",
        "\n",
        "def read_video_cv2(path, max_frames=240, sample_frames=120):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    frames = []\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frames.append(frame)\n",
        "    cap.release()\n",
        "\n",
        "    total_frames = len(frames)\n",
        "    if total_frames == 0:\n",
        "        raise RuntimeError(f\"Cannot read video {path}\")\n",
        "\n",
        "    # Â¶ÇÊûúÂΩ±ÁâáÂπÄÊï∏‰∏çÂ§†ÔºåË£úÊúÄÂæå‰∏ÄÂπÄ\n",
        "    while len(frames) < max_frames:\n",
        "        frames.append(frames[-1].copy())\n",
        "\n",
        "    frames = frames[:max_frames]  # Á¢∫‰øùÈï∑Â∫¶‰∏çË∂ÖÈÅémax_frames\n",
        "\n",
        "    # Á≠âË∑ùÊäΩÊ®£Êàê sample_frames ÂπÄ\n",
        "    indices = np.linspace(0, max_frames - 1, sample_frames).astype(int)\n",
        "    sampled_frames = [frames[i] for i in indices]\n",
        "\n",
        "    video_np = np.stack(sampled_frames, axis=0)  # (T, H, W, C)\n",
        "    video_t = torch.from_numpy(video_np).permute(3, 0, 1, 2)  # (C, T, H, W)\n",
        "    return video_t\n",
        "\n",
        "# Ë≥áÊñôÊ®°Âûã\n",
        "class Normalize(torch.nn.Module):\n",
        "    def __init__(self, mean, std):\n",
        "        super().__init__()\n",
        "        self.mean = torch.tensor(mean).view(-1, 1, 1, 1)\n",
        "        self.std = torch.tensor(std).view(-1, 1, 1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return (x - self.mean) / self.std\n",
        "\n",
        "# üîß ÂÆâÂÖ®Ë≥áÊñôÂ¢ûÂº∑\n",
        "class SafeVideoAugmentation:\n",
        "    def __init__(self, resize=(224, 224), apply_blur_prob=0.3, apply_brightness_prob=0.3):\n",
        "        self.resize = resize\n",
        "        self.to_tensor = transforms.ToTensor()\n",
        "        self.apply_blur_prob = apply_blur_prob\n",
        "        self.apply_brightness_prob = apply_brightness_prob\n",
        "        self.normalize = Normalize(mean=[0.45, 0.45, 0.45], std=[0.225, 0.225, 0.225])\n",
        "\n",
        "    def __call__(self, frames):\n",
        "        augmented = []\n",
        "        apply_blur = random.random() < self.apply_blur_prob\n",
        "        apply_brightness = random.random() < self.apply_brightness_prob\n",
        "        brightness_factor = random.uniform(0.8, 1.2)\n",
        "\n",
        "        for frame in frames:\n",
        "            frame = cv2.resize(frame, self.resize)\n",
        "            pil_frame = Image.fromarray(frame)\n",
        "\n",
        "            if apply_blur:\n",
        "                pil_frame = pil_frame.filter(ImageFilter.GaussianBlur(radius=1.5))  # radius ÂèØ‰ª•Ë™øÊï¥Ê®°Á≥äÁ®ãÂ∫¶\n",
        "            if apply_brightness:\n",
        "                pil_frame = transforms.functional.adjust_brightness(pil_frame, brightness_factor)\n",
        "\n",
        "            tensor_frame = self.to_tensor(pil_frame)\n",
        "            augmented.append(tensor_frame)\n",
        "        augmented_tensor = torch.stack(augmented) # (T, C, H, W)\n",
        "        augmented_tensor = augmented_tensor.permute(1, 0, 2, 3)  # (C, T, H, W)\n",
        "        augmented_tensor = self.normalize(augmented_tensor)\n",
        "        return augmented_tensor\n",
        "class VideoDataset(Dataset):\n",
        "    def __init__(self, csv_path, video_dir, original_frames=240, sample_frames=120, transform=None):\n",
        "        self.video_dir = video_dir\n",
        "        self.original_frames = original_frames  # ÂΩ±ÁâáÂéüÂßãÈï∑Â∫¶(ÊúÄÂ§ßÂπÄÊï∏)\n",
        "        self.sample_frames = sample_frames      # Ë¶ÅÁ≠âË∑ùÊäΩÊ®£ÊàêÂ§öÂ∞ëÂπÄ\n",
        "        self.transform = transform or SafeVideoAugmentation()\n",
        "        self.data = pd.read_csv(csv_path)\n",
        "        # ÁØ©ÈÅ∏Âá∫Â≠òÂú®ÁöÑÂΩ±ÁâáË∑ØÂæë\n",
        "        def file_exists(filename):\n",
        "            return os.path.isfile(os.path.join(video_dir, filename))\n",
        "\n",
        "        mask = self.data['filename'].apply(file_exists)\n",
        "        filtered_data = self.data[mask].reset_index(drop=True)\n",
        "        num_removed = len(self.data) - len(filtered_data)\n",
        "        if num_removed > 0:\n",
        "            print(f\"Warning: removed {num_removed} entries because video files not found\")\n",
        "        self.data = filtered_data\n",
        "\n",
        "        self.data['label'] = self.data['description'].str.contains('strike', case=False).astype(int)\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        video_path = os.path.join(self.video_dir, row['filename'])\n",
        "        label = row['label']\n",
        "\n",
        "        video = read_video_cv2(video_path, self.original_frames, self.sample_frames)\n",
        "        video = video.permute(1, 2, 3, 0).numpy()  # (T,H,W,C)\n",
        "        if self.transform:\n",
        "            video = self.transform(video)\n",
        "        return video, label\n",
        "\n",
        "# ------- 1. Ë®ìÁ∑¥ÂáΩÊï∏ --------\n",
        "def train_one_epoch(model, loader, criterion, optimizer, device, scaler):\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "\n",
        "    for videos, labels in tqdm(loader, desc=\"Training\", leave=False):\n",
        "        videos, labels = videos.to(device), labels.to(device).long()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            outputs = model(videos)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    return total_loss / len(loader), correct / total\n",
        "\n",
        "# ------- 2. È©óË≠âÂáΩÊï∏ --------\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        with torch.cuda.amp.autocast():\n",
        "            for videos, labels in tqdm(loader, desc=\"Validating\", leave=False):\n",
        "                videos, labels = videos.to(device), labels.to(device).long()\n",
        "                outputs = model(videos)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                preds = outputs.argmax(dim=1)\n",
        "                correct += (preds == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "    return total_loss / len(loader), correct / total\n",
        "\n",
        "# ------- 3. ‰∏ªË®ìÁ∑¥ÊµÅÁ®ã --------\n",
        "def main():\n",
        "    batch_size = 10      # ÊäΩÊ®£Êàê120ÂπÄÔºåÂèØ‰ª•Ë©¶ËëóÂä†Â§ßbatch_size\n",
        "    num_epochs = 20\n",
        "    original_frames = 240\n",
        "    sample_frames = 120\n",
        "    val_split = 0.2\n",
        "    lr = 1e-4\n",
        "    dataset_paths = [\n",
        "        {\n",
        "            \"csv_path\": \"/content/drive/MyDrive/Baseball Movies/CH_videos_4s/CH.csv\",\n",
        "            \"video_dir\": \"/content/drive/MyDrive/Baseball Movies/CH_videos_4s\"\n",
        "        },\n",
        "        {\n",
        "            \"csv_path\": \"/content/drive/MyDrive/Baseball Movies/FF_videos_4s/FF.csv\",\n",
        "            \"video_dir\": \"/content/drive/MyDrive/Baseball Movies/FF_videos_4s\"\n",
        "        },\n",
        "        {\n",
        "            \"csv_path\": \"/content/drive/MyDrive/Baseball Movies/SL_videos_4s/SL.csv\",\n",
        "            \"video_dir\": \"/content/drive/MyDrive/Baseball Movies/SL_videos_4s\"\n",
        "        },\n",
        "    ]\n",
        "    datasets = []\n",
        "    for item in dataset_paths:\n",
        "        dataset = VideoDataset(\n",
        "            csv_path=item[\"csv_path\"],\n",
        "            video_dir=item[\"video_dir\"],\n",
        "            original_frames=original_frames,\n",
        "            sample_frames=sample_frames,\n",
        "            transform=None  # ÂÖà‰∏çÂä† transformÔºåÁ®çÂæåË®≠ÁΩÆ\n",
        "        )\n",
        "        datasets.append(dataset)\n",
        "\n",
        "    full_dataset = ConcatDataset(datasets)\n",
        "\n",
        "    # 1. ÂàÜÂâ≤ index\n",
        "    val_split = 0.2\n",
        "    dataset_len = len(full_dataset)\n",
        "    indices = list(range(dataset_len))\n",
        "    split = int(val_split * dataset_len)\n",
        "    random.shuffle(indices)\n",
        "    train_indices, val_indices = indices[split:], indices[:split]\n",
        "\n",
        "    # 2. Âª∫Á´ã train/val datasetÔºå‰∏¶Ë§áË£ΩÂéüÂßã dataset\n",
        "    train_dataset = copy.deepcopy(full_dataset)\n",
        "    val_dataset = copy.deepcopy(full_dataset)\n",
        "\n",
        "    # 3. Ë®≠ÂÆö transform ÂàÜÂà•Áµ¶ train/val dataset\n",
        "    for d in train_dataset.datasets:\n",
        "        d.transform = SafeVideoAugmentation()\n",
        "\n",
        "    for d in val_dataset.datasets:\n",
        "        d.transform = SafeVideoAugmentation(apply_blur_prob=0.0, apply_brightness_prob=0.0)\n",
        "\n",
        "    # 4. Âª∫Á´ã Subset\n",
        "    train_set = Subset(train_dataset, train_indices)\n",
        "    val_set = Subset(val_dataset, val_indices)\n",
        "\n",
        "    # 5. Âª∫Á´ã dataloader\n",
        "    batch_size = 10\n",
        "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "    model = x3d_xs(pretrained=True)\n",
        "    model.blocks[-1].proj = nn.Linear(model.blocks[-1].proj.in_features, 2)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Âä†ÂÖ• AMP ÁöÑ GradScaler\n",
        "    scaler = torch.amp.GradScaler('cuda')\n",
        "\n",
        "    best_val_acc = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device, scaler)\n",
        "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.4f}\")\n",
        "\n",
        "        # ‰øùÂ≠òÂà∞Èõ≤Á´Ø ÂëΩÂêçÁÇ∫ epochÂíåval_acc\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            # ÂãïÊÖãÂëΩÂêçÊ®°ÂûãÂÑ≤Â≠òÊ™îÊ°àÂà∞google drive MyDrive\n",
        "            save_dir = \"/content/drive/MyDrive\"\n",
        "            os.makedirs(save_dir, exist_ok=True)\n",
        "            filename = f\"epoch_{epoch+1}_valacc_{val_acc:.4f}.pt\"\n",
        "            model_save_path = os.path.join(save_dir, filename)\n",
        "            torch.save(model.state_dict(), model_save_path)\n",
        "            print(f\"‚úÖ Saved best model to Google Drive: {model_save_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RUnMT7mfSnhh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}